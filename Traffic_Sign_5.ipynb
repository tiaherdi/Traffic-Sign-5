{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Traffic Sign_5",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tiaherdi/Traffic-Sign-5/blob/main/Traffic_Sign_5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gxKoWmqr9b1R"
      },
      "outputs": [],
      "source": [
        "# Import library\n",
        "\n",
        "import tensorflow as tf\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import numpy as np # linear algebra\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns # graph"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Read file from dataset via github \n",
        "dataset = pd.read_csv('https://raw.githubusercontent.com/tiaherdi/Traffic-Sign-5/main/covid_impact_on_airport_traffic.csv?token=GHSAT0AAAAAABXA5PPA37IFB23TYU6SFZYUYXBICBQ',index_col='Date',parse_dates=['Date'])\n",
        "dataset.head()"
      ],
      "metadata": {
        "id": "JnKuuGKkDNd5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# a. Exploratory Data Analysis (EDA)"
      ],
      "metadata": {
        "id": "IhRYDJsvfFNP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Data distribution based on country\n",
        "data_per_negara = dataset.groupby(\"Country\").count()\n",
        "data_per_negara"
      ],
      "metadata": {
        "id": "1AwqPhQME0EY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Five earliest data sort by Date\n",
        "data = dataset.sort_values(by=\"Date\")\n",
        "data.head()"
      ],
      "metadata": {
        "id": "K7tqHvvEIIQi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Five latest data after sorted by Date\n",
        "data.tail()"
      ],
      "metadata": {
        "id": "zzfZF7wlIOd9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Describing descriptive statistic on Percent of Baseline\n",
        "data.describe()"
      ],
      "metadata": {
        "id": "gSLPKQQkNLkv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature selection -> get data needed\n",
        "data = dataset.drop(['AggregationMethod', 'Version', 'ISO_3166_2', 'Geography', 'Centroid', 'State', 'City'], \n",
        "                         axis=1)\n",
        "data = data.sort_values(by=\"Date\")\n",
        "data.head()"
      ],
      "metadata": {
        "id": "nezt4q41I1Id"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating DataFrame based on country\n",
        "data_per_negara = data.groupby('Country')\n",
        "australia = data_per_negara.get_group('Australia')\n",
        "canada = data_per_negara.get_group('Canada')\n",
        "chile = data_per_negara.get_group('Chile')\n",
        "us  = data_per_negara.get_group('United States of America (the)')"
      ],
      "metadata": {
        "id": "PBoT5rktNcil"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualization Percent of Baseline based on country and date"
      ],
      "metadata": {
        "id": "cgL-ocAIgzxv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualization by country\n",
        "australia.plot(title='Australia')\n",
        "canada.plot(title='Canada')\n",
        "chile.plot(title='Chile')\n",
        "us.plot(title='US')"
      ],
      "metadata": {
        "id": "I5V39Leogjk4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# b. Time-Series Analysis"
      ],
      "metadata": {
        "id": "2kfsephPhHrd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Percent of Baseline per-country\n",
        "baseline_au = australia.PercentOfBaseline.sort_index()\n",
        "baseline_ca = canada.PercentOfBaseline.sort_index()\n",
        "baseline_cl = chile.PercentOfBaseline.sort_index()\n",
        "baseline_us = us.PercentOfBaseline.sort_index()"
      ],
      "metadata": {
        "id": "0wBjWoMMhId6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start = pd.to_datetime('2020-03-16')\n",
        "end = pd.to_datetime('2020-12-02')\n",
        "baseline_au = baseline_au[start:end]\n",
        "print('Sample of Time Series Data (Australia):')\n",
        "baseline_au.head()"
      ],
      "metadata": {
        "id": "KSdUf7q1hBlT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start = pd.to_datetime('2020-03-16')\n",
        "end = pd.to_datetime('2020-12-02')\n",
        "baseline_ca = baseline_ca[start:end]\n",
        "print('Sample of Time Series Data (Canada):')\n",
        "baseline_ca.head()"
      ],
      "metadata": {
        "id": "IKIxsMrknRBY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start = pd.to_datetime('2020-03-16')\n",
        "end = pd.to_datetime('2020-12-02')\n",
        "baseline_cl = baseline_cl[start:end]\n",
        "print('Sample of Time Series Data (Chile):')\n",
        "baseline_cl.head()"
      ],
      "metadata": {
        "id": "JP15jFEXuda8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start = pd.to_datetime('2020-03-16')\n",
        "end = pd.to_datetime('2020-12-02')\n",
        "baseline_us = baseline_us[start:end]\n",
        "print('Sample of Time Series Data (US):')\n",
        "baseline_us.head()"
      ],
      "metadata": {
        "id": "ycoXvcAiuiJf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plotting Time-Series"
      ],
      "metadata": {
        "id": "lZmGiOiCv52v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting time series\n",
        "# Australia\n",
        "plt.figure(figsize=(15, 8))\n",
        "plt.plot(baseline_au, color='blue')\n",
        "plt.title('Percent of Baseline of Australia', weight='bold', fontsize=15)\n",
        "plt.xlabel('Date', fontsize=14)\n",
        "plt.ylabel('Percent of Baseline', fontsize=14)\n",
        "plt.xticks(weight='bold', fontsize=12, rotation=45)\n",
        "plt.yticks(weight='bold', fontsize=12)\n",
        "plt.grid(color = 'y', linewidth = 0.5)"
      ],
      "metadata": {
        "id": "lYLDDVQUunji"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Canada\n",
        "plt.figure(figsize=(15, 8))\n",
        "plt.plot(baseline_ca, color='blue')\n",
        "plt.title('Percent of Baseline of Canada', weight='bold', fontsize=15)\n",
        "plt.xlabel('Date', fontsize=14)\n",
        "plt.ylabel('Percent of Baseline', fontsize=14)\n",
        "plt.xticks(weight='bold', fontsize=12, rotation=45)\n",
        "plt.yticks(weight='bold', fontsize=12)\n",
        "plt.grid(color = 'y', linewidth = 0.5)"
      ],
      "metadata": {
        "id": "h9n7MY_-vttJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Chile\n",
        "plt.figure(figsize=(15, 8))\n",
        "plt.plot(baseline_cl, color='blue')\n",
        "plt.title('Percent of Baseline of Chile', weight='bold', fontsize=15)\n",
        "plt.xlabel('Date', fontsize=14)\n",
        "plt.ylabel('Percent of Baseline', fontsize=14)\n",
        "plt.xticks(weight='bold', fontsize=12, rotation=45)\n",
        "plt.yticks(weight='bold', fontsize=12)\n",
        "plt.grid(color = 'y', linewidth = 0.5)"
      ],
      "metadata": {
        "id": "xe4jtHV9wITq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# US\n",
        "plt.figure(figsize=(15, 8))\n",
        "plt.plot(baseline_us, color='blue')\n",
        "plt.title('Percent of Baseline of US', weight='bold', fontsize=15)\n",
        "plt.xlabel('Date', fontsize=14)\n",
        "plt.ylabel('Percent of Baseline', fontsize=14)\n",
        "plt.xticks(weight='bold', fontsize=12, rotation=45)\n",
        "plt.yticks(weight='bold', fontsize=12)\n",
        "plt.grid(color = 'y', linewidth = 0.5)"
      ],
      "metadata": {
        "id": "suBjxlmAwRep"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# c. Replacing Missing Data with Estimated Value"
      ],
      "metadata": {
        "id": "Ezji0zufwgv-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Australia\n",
        "range = pd.date_range(start='2020-03-16', end='2020-12-02', freq='D')\n",
        "ts = baseline_au.index\n",
        "diff_dates = range.difference(ts)\n",
        "\n",
        "td = pd.Timedelta(1, 'd') # 1 day\n",
        "for date in diff_dates:\n",
        "  prev_val = baseline_au[date-td] # Take earlier value\n",
        "  baseline_au[date] = prev_val # Throw earlier value\n",
        "\n",
        "baseline_au.sort_index(inplace=True)\n",
        "# Set time index frequency as daily\n",
        "baseline_au.freq=\"D\""
      ],
      "metadata": {
        "id": "d0TBM_qlwtvj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Canada\n",
        "range = pd.date_range(start='2020-03-16', end='2020-12-02', freq='D')\n",
        "ts = baseline_ca.index\n",
        "diff_dates = range.difference(ts)\n",
        "\n",
        "td = pd.Timedelta(1, 'd') # 1 day\n",
        "for date in diff_dates:\n",
        "  prev_val = baseline_ca[date-td] # Take earlier value\n",
        "  baseline_ca[date] = prev_val # Throw earlier value\n",
        "\n",
        "baseline_ca.sort_index(inplace=True)\n",
        "# Set time index frequency as daily\n",
        "baseline_ca.freq=\"D\""
      ],
      "metadata": {
        "id": "JKDfVekLxFYl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Chile\n",
        "range = pd.date_range(start='2020-03-16', end='2020-12-02', freq='D')\n",
        "ts = baseline_cl.index\n",
        "diff_dates = range.difference(ts)\n",
        "\n",
        "td = pd.Timedelta(1, 'd') # 1 day\n",
        "for date in diff_dates:\n",
        "  prev_val = baseline_cl[date-td] # Take earlier value\n",
        "  baseline_cl[date] = prev_val # Throw earlier value\n",
        "\n",
        "baseline_cl.sort_index(inplace=True)\n",
        "# Set time index frequency as daily\n",
        "baseline_cl.freq=\"D\""
      ],
      "metadata": {
        "id": "m3z03T5-xsEF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# US\n",
        "range = pd.date_range(start='2020-03-16', end='2020-12-02', freq='D')\n",
        "ts = baseline_us.index\n",
        "diff_dates = range.difference(ts)\n",
        "\n",
        "td = pd.Timedelta(1, 'd') # 1 day\n",
        "for date in diff_dates:\n",
        "  prev_val = baseline_us[date-td] # Take earlier value\n",
        "  baseline_us[date] = prev_val # Throw earlier value\n",
        "\n",
        "baseline_us.sort_index(inplace=True)\n",
        "# Set time index frequency as daily\n",
        "baseline_us.freq=\"D\""
      ],
      "metadata": {
        "id": "V_5BwPqyx32q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# d. Split Data for Training and **Testing**"
      ],
      "metadata": {
        "id": "8IH1GuW0yHaw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Australia**"
      ],
      "metadata": {
        "id": "5bOhXnCMygkm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We use 80% of data Percent of Baseline for train data, 20% for test data\n",
        "split_time = int(0.8 * len(baseline_au))\n",
        "\n",
        "train_au = baseline_au[:split_time]\n",
        "test_au = baseline_au[split_time:]"
      ],
      "metadata": {
        "id": "38UACrxQx9nN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display a plot of train and test split\n",
        "fig = plt.figure(figsize=(20,8))\n",
        "plt.plot(train_au, color='blue', label='Training')\n",
        "plt.plot(test_au, color='red', label='Testing')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Percent of Baseline')\n",
        "plt.title('Australia')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "yMHaAJpyykQX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Canada**"
      ],
      "metadata": {
        "id": "9XD81H4Py9tP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We use 80% of data Percent of Baseline for train data, 20% for test data\n",
        "split_time = int(0.8 * len(baseline_ca))\n",
        "\n",
        "train_ca = baseline_ca[:split_time]\n",
        "test_ca= baseline_ca[split_time:]"
      ],
      "metadata": {
        "id": "_2JXVLl2yrFZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display a plot of train and test split\n",
        "fig = plt.figure(figsize=(20,8))\n",
        "plt.plot(train_ca, color='blue', label='Training')\n",
        "plt.plot(test_ca, color='red', label='Testing')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Percent of Baseline')\n",
        "plt.title('Canada')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "V4LacEglzCbj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Chile**"
      ],
      "metadata": {
        "id": "SItTZF2AzHTw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We use 80% of data Percent of Baseline for train data, 20% for test data\n",
        "split_time = int(0.8 * len(baseline_cl))\n",
        "\n",
        "train_cl = baseline_cl[:split_time]\n",
        "test_cl = baseline_cl[split_time:]"
      ],
      "metadata": {
        "id": "LgyQZthzzFQm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display a plot of train and test split\n",
        "fig = plt.figure(figsize=(20,8))\n",
        "plt.plot(train_cl, color='blue', label='Training')\n",
        "plt.plot(test_cl, color='red', label='Testing')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Percent of Baseline')\n",
        "plt.title('Chile')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "NU8v6dNGzJ6T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**US**"
      ],
      "metadata": {
        "id": "i1IxC0_tzSoV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We use 80% of data Percent of Baseline for train data, 20% for test data\n",
        "split_time = int(0.8 * len(baseline_us))\n",
        "\n",
        "train_us = baseline_us[:split_time]\n",
        "test_us = baseline_us[split_time:]"
      ],
      "metadata": {
        "id": "5KidSmLlzLsR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display a plot of train and test split\n",
        "fig = plt.figure(figsize=(20,8))\n",
        "plt.plot(train_us, color='blue', label='Training')\n",
        "plt.plot(test_us, color='red', label='Testing')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Percent of Baseline')\n",
        "plt.title('Chile')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3eCCKhg6zXnc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# e. Window Dataset"
      ],
      "metadata": {
        "id": "7EtH31UGz0w5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Australia\n",
        "# Creates a windowed dataset\n",
        "WINDOW = 15 \n",
        "\n",
        "train_data_au = tf.data.Dataset.from_tensor_slices(train_au.values) # convert to TensorSliceDataset\n",
        "train_data_au = train_data_au.window(WINDOW+1, shift=1, drop_remainder=True) # takes window size+1\n",
        "train_data_au = train_data_au.flat_map(lambda x: x.batch(WINDOW+1)) # flattens windowed data\n",
        "train_data_au = train_data_au.map(lambda x: (x[:-1], x[-1])) # creates features and target tuple\n",
        "\n",
        "train_data_au = train_data_au.shuffle(1_000) # shuffles dataset Australia\n",
        "\n",
        "train_data_au = train_data_au.batch(32).prefetch(1) # creates batches of windows"
      ],
      "metadata": {
        "id": "DgiTX11Szu4k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Canada\n",
        "# Creates a windowed dataset\n",
        "WINDOW = 15 \n",
        "\n",
        "train_data_ca = tf.data.Dataset.from_tensor_slices(train_ca.values) # convert to TensorSliceDataset\n",
        "train_data_ca = train_data_ca.window(WINDOW+1, shift=1, drop_remainder=True) # takes window size+1\n",
        "train_data_ca = train_data_ca.flat_map(lambda x: x.batch(WINDOW+1)) # flattens windowed data\n",
        "train_data_ca = train_data_ca.map(lambda x: (x[:-1], x[-1])) # creates features and target tuple\n",
        "\n",
        "train_data_ca = train_data_ca.shuffle(1_000) # shuffles dataset Canada\n",
        "\n",
        "train_data_ca = train_data_ca.batch(32).prefetch(1) # creates batches of windows"
      ],
      "metadata": {
        "id": "ugxVvaJd2VbD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Chile\n",
        "# Creates a windowed dataset\n",
        "WINDOW = 15 \n",
        "\n",
        "train_data_cl = tf.data.Dataset.from_tensor_slices(train_cl.values) # convert to TensorSliceDataset\n",
        "train_data_cl = train_data_cl.window(WINDOW+1, shift=1, drop_remainder=True) # takes window size+1\n",
        "train_data_cl = train_data_cl.flat_map(lambda x: x.batch(WINDOW+1)) # flattens windowed data\n",
        "train_data_cl = train_data_cl.map(lambda x: (x[:-1], x[-1])) # creates features and target tuple\n",
        "\n",
        "train_data_cl = train_data_cl.shuffle(1_000) # shuffles dataset Chile\n",
        "\n",
        "train_data_cl = train_data_cl.batch(32).prefetch(1) # creates batches of windows"
      ],
      "metadata": {
        "id": "Jg4d-0io2u_L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# US\n",
        "# Creates a windowed dataset\n",
        "WINDOW = 15 \n",
        "\n",
        "train_data_us = tf.data.Dataset.from_tensor_slices(train_us.values) # convert to TensorSliceDataset\n",
        "train_data_us = train_data_us.window(WINDOW+1, shift=1, drop_remainder=True) # takes window size+1\n",
        "train_data_us = train_data_us.flat_map(lambda x: x.batch(WINDOW+1)) # flattens windowed data\n",
        "train_data_us = train_data_us.map(lambda x: (x[:-1], x[-1])) # creates features and target tuple\n",
        "\n",
        "train_data_us = train_data_us.shuffle(1_000) # shuffles dataset US\n",
        "\n",
        "train_data_us = train_data_us.batch(32).prefetch(1) # creates batches of windows"
      ],
      "metadata": {
        "id": "NbrRjNyV28YZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# f. Callback"
      ],
      "metadata": {
        "id": "uh-mqnVX3Tle"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import Callback\n",
        "\n",
        "class CustomCallback(Callback):\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        if logs.get('mae') < 10.0:\n",
        "            print(\"MAE under 10.0... Stopping training\")\n",
        "            self.model.stop_training = True\n",
        "\n",
        "my_callback = CustomCallback()"
      ],
      "metadata": {
        "id": "c-RJyivA1XdN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import LearningRateScheduler\n",
        "\n",
        "#creates a function that updates the learning rate based on the epoch number\n",
        "def scheduler(epoch, lr):\n",
        "    if epoch < 2:\n",
        "        return 0.01\n",
        "    else:\n",
        "        return lr * 0.99\n",
        "\n",
        "lr_scheduler = LearningRateScheduler(scheduler)"
      ],
      "metadata": {
        "id": "ieb4b5tA2BR0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# g. Build the Model"
      ],
      "metadata": {
        "id": "inmqKLmV2IYe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Australia**"
      ],
      "metadata": {
        "id": "1u0DberG41d-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv1D, Dense, LSTM, Dropout, Lambda, Bidirectional\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.losses import Huber\n",
        "\n",
        "mixed_model = Sequential([\n",
        "    # add extra axis to input data\n",
        "    Lambda(lambda x: tf.expand_dims(x, axis=-1), input_shape=[WINDOW]), Conv1D(filters=64, kernel_size=3, strides=1,\n",
        "           padding='causal', activation='relu'),\n",
        "    Bidirectional(LSTM(128, return_sequences=True)),\n",
        "    Bidirectional(LSTM(128)),\n",
        "    Dropout(0.3),\n",
        "    Dense(512, activation='relu'),\n",
        "    Dropout(0.4),\n",
        "    Dense(1)\n",
        "])"
      ],
      "metadata": {
        "id": "WWmUmR2-2FHu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mixed_model.compile(\n",
        "    loss=Huber(),\n",
        "    optimizer=Adam(),\n",
        "    metrics=['mae']\n",
        ")\n",
        "\n",
        "mixed_model.summary()"
      ],
      "metadata": {
        "id": "pS4N58rS4OIZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Trains Mixed Model\n",
        "mixed_history = mixed_model.fit(\n",
        "    train_data_au,\n",
        "    epochs=30,\n",
        "    callbacks=[lr_scheduler, my_callback],\n",
        "    verbose=0\n",
        ")"
      ],
      "metadata": {
        "id": "hGWkwbEx4PVb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plots training history\n",
        "# Plots history of model training\n",
        "plt.rcParams[\"figure.figsize\"] = (15,5)\n",
        "fig, axs = plt.subplots(1, 2)\n",
        "\n",
        "axs[0].plot(mixed_history.history['loss'], color='red')\n",
        "axs[0].set_xlabel('Epoch')\n",
        "axs[0].set_ylabel('Loss')\n",
        "axs[0].set_title('Training Loss')\n",
        "\n",
        "axs[1].plot(mixed_history.history['mae'])\n",
        "axs[1].set_xlabel('Epoch')\n",
        "axs[1].set_ylabel('MAE')\n",
        "axs[1].set_title('Training MAE')\n",
        "\n",
        "fig.text(0.425,1, 'MIXED MODEL', {'size':25})\n",
        "plt.show()\n",
        "\n",
        "print(\"\\t\\t\\t\\t\\tFINAL LOSS: {} | FINAL MAE: {}\".format(\n",
        "                                                round(mixed_history.history['loss'][-1], 2),\n",
        "                                                 round(mixed_history.history['mae'][-1], 2)))\n"
      ],
      "metadata": {
        "id": "QVZYR6z34SEC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Canada**"
      ],
      "metadata": {
        "id": "lyFpw59e5o3y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv1D, Dense, LSTM, Dropout, Lambda, Bidirectional\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.losses import Huber\n",
        "\n",
        "mixed_model = Sequential([\n",
        "    # add extra axis to input data\n",
        "    Lambda(lambda x: tf.expand_dims(x, axis=-1), input_shape=[WINDOW]), Conv1D(filters=64, kernel_size=3, strides=1,\n",
        "           padding='causal', activation='relu'),\n",
        "    Bidirectional(LSTM(128, return_sequences=True)),\n",
        "    Bidirectional(LSTM(128)),\n",
        "    Dropout(0.3),\n",
        "    Dense(512, activation='relu'),\n",
        "    Dropout(0.4),\n",
        "    Dense(1)\n",
        "])"
      ],
      "metadata": {
        "id": "JpPQsdIe5qec"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mixed_model.compile(\n",
        "    loss=Huber(),\n",
        "    optimizer=Adam(),\n",
        "    metrics=['mae']\n",
        ")\n",
        "\n",
        "mixed_model.summary()"
      ],
      "metadata": {
        "id": "XbiDzWw1509C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Trains Mixed Model\n",
        "mixed_history = mixed_model.fit(\n",
        "    train_data_ca,\n",
        "    epochs=100,\n",
        "    callbacks=[lr_scheduler, my_callback],\n",
        "    verbose=0\n",
        ")"
      ],
      "metadata": {
        "id": "YaGgFoJv55Kl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plots training history\n",
        "# Plots history of model training\n",
        "plt.rcParams[\"figure.figsize\"] = (15,5)\n",
        "fig, axs = plt.subplots(1, 2)\n",
        "\n",
        "axs[0].plot(mixed_history.history['loss'], color='red')\n",
        "axs[0].set_xlabel('Epoch')\n",
        "axs[0].set_ylabel('Loss')\n",
        "axs[0].set_title('Training Loss')\n",
        "\n",
        "axs[1].plot(mixed_history.history['mae'])\n",
        "axs[1].set_xlabel('Epoch')\n",
        "axs[1].set_ylabel('MAE')\n",
        "axs[1].set_title('Training MAE')\n",
        "\n",
        "fig.text(0.425,1, 'MIXED MODEL', {'size':25})\n",
        "plt.show()\n",
        "\n",
        "print(\"\\t\\t\\t\\t\\tFINAL LOSS: {} | FINAL MAE: {}\".format(\n",
        "                                                round(mixed_history.history['loss'][-1], 2),\n",
        "                                                 round(mixed_history.history['mae'][-1], 2)))\n"
      ],
      "metadata": {
        "id": "SBNcqaHK58dp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# h. Forecast Data"
      ],
      "metadata": {
        "id": "5ahj2lp74eia"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "pODtzgBn57kC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "g7W7MoV754s3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "CKzy8Cc750SL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Australia\n",
        "# Chunck of data to be windowed so that each window associated to a value in test set\n",
        "forecast_data_au = train_au[-WINDOW:].append(test_au[:-1]).values"
      ],
      "metadata": {
        "id": "UdorRUYZ4YOi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_forecasts = {'MIXED MODEL': []}\n",
        "\n",
        "#converts values to TensorSliceDataset\n",
        "test_data = tf.data.Dataset.from_tensor_slices(forecast_data) \n",
        "    #takes window size  slices of the dataset\n",
        "test_data = test_data.window(WINDOW, shift=1, drop_remainder=True)\n",
        "    #flattens windowed data by batching \n",
        "test_data = test_data.flat_map(lambda x: x.batch(WINDOW+1))\n",
        "    #creates batches of windows\n",
        "test_data = test_data.batch(32).prefetch(1)\n",
        "    #gets model prediction \n",
        "preds = mixed_model.predict(test_data)\n",
        "    #append to forecast dict\n",
        "model_forecasts['MIXED MODEL'].append(preds)"
      ],
      "metadata": {
        "id": "9ivKsQte4pHH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}